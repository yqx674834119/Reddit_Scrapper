import json
import uuid
import time
import os
import openai
import requests
from config.config_loader import get_config
from scheduler.cost_tracker import add_cost
from utils.logger import setup_logger

log = setup_logger()
config = get_config()

BATCH_FILE_PATH = "data/batch_requests.jsonl"
RESPONSE_DIR = "data/batch_responses"

def generate_batch_payload(requests: list[dict], model: str) -> str:
    """Create a JSONL file from prompts for OpenAI batch processing."""
    os.makedirs(RESPONSE_DIR, exist_ok=True)
    job_id = str(uuid.uuid4())
    path = f"{RESPONSE_DIR}/batch_{job_id}.jsonl"

    with open(path, "w", encoding="utf-8") as f:
        for prompt in requests:
            f.write(json.dumps({
                "custom_id": prompt.get("id", str(uuid.uuid4())),
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": model,
                    "messages": prompt["messages"],
                    "temperature": 0,
                }
            }) + "\n")

    log.info(f"Batch payload generated at: {path} with {len(requests)} entries")
    return path

def submit_batch_job(file_path: str, endpoint: str = "/v1/chat/completions") -> str:
    """Uploads file and submits a batch job to OpenAI."""
    uploaded_file = openai.files.create(file=open(file_path, "rb"), purpose="batch")
    log.info(f"Uploaded file for batch: {uploaded_file.id}")

    batch = openai.batches.create(
        input_file_id=uploaded_file.id,
        endpoint=endpoint,
        completion_window="24h",
    )
    log.info(f"Submitted batch job: {batch.id}")
    return batch.id

def poll_batch_status(batch_id: str):
    """Polls batch status every 15s until it's complete or failed."""
    while True:
        batch = openai.batches.retrieve(batch_id)
        log.info(f"Batch {batch_id} status: {batch.status}")
        if batch.status in {"completed", "failed", "expired"}:
            return batch
        time.sleep(15)

def download_batch_results(batch_id: str, save_path: str):
    """Downloads and stores the results of a completed batch job."""
    batch = openai.batches.retrieve(batch_id)
    if batch.status != "completed":
        raise RuntimeError(f"Batch {batch_id} not completed.")

    output_file_id = batch.output_file_id  # ‚Üê updated attribute
    if not output_file_id:
        raise RuntimeError(f"No output file found for batch {batch_id}.")

    output_file = openai.files.retrieve(output_file_id)
    response = openai.files.content(output_file_id)

    with open(save_path, "wb") as f:
        f.write(response.read())

    log.info(f"Saved results to {save_path}")

def add_estimated_batch_cost(requests: list[dict], model: str):
    """Estimate and record the cost of the batch job using accurate pricing."""
    # Per 1M token pricing in USD
    pricing = {
        "gpt-4.1": {"input": 0.0020, "output": 0.0080},
        "gpt-4.1-mini": {"input": 0.0004, "output": 0.0016},
        "gpt-4o-mini": {"input": 0.00015, "output": 0.0006},
    }

    discount_factor = 0.5  # 50% discount for batch jobs

    # Default fallback pricing
    model_pricing = pricing.get(model, {"input": 0.0010, "output": 0.0010})

    input_tokens = sum(req.get("meta", {}).get("estimated_tokens", 300) for req in requests)
    output_tokens = len(requests) * 300  # Assumes ~300 output tokens per completion

    input_cost = (input_tokens / 200_000) * model_pricing["input"] * discount_factor
    output_cost = (output_tokens / 500_000) * model_pricing["output"] * discount_factor
    estimated_cost = input_cost + output_cost

    log.info(f"Estimated cost for batch (input + output): ${estimated_cost:.4f}")
    add_cost(estimated_cost)
